"""Additional targeted tests to increase coverage further.

These tests target specific remaining uncovered lines.
"""

import tempfile
import unittest
from pathlib import Path
from unittest.mock import Mock, patch

from kirkham import KirkhamParser, ParserConfig
from kirkham.classifier import PartOfSpeechClassifier
from kirkham.formatter import OutputFormatter
from kirkham.lexicon import Lexicon
from kirkham.models import Flag, ParseResult, Span, Token
from kirkham.types import (
    Case, Gender, Number, PartOfSpeech, Person, RuleID, SentenceType, Tense,
    Voice,
)
from kirkham.validator import GrammarRuleValidator


class TestAdditionalCoverage(unittest.TestCase):
    """Additional test suite for increased coverage."""

    def setUp(self):
        """Set up test fixtures."""
        self.parser = KirkhamParser()
        self.validator = GrammarRuleValidator(Lexicon())
        self.formatter = OutputFormatter()
        self.classifier = PartOfSpeechClassifier(Lexicon())

    def create_token(self, text: str, pos: PartOfSpeech, **kwargs) -> Token:
        """Create a test token."""
        defaults = {
            "lemma": text.lower(),
            "start": 0,
            "end": len(text),
            "case": None,
            "gender": None,
            "number": None,
            "person": None,
            "features": {},
        }
        defaults.update(kwargs)
        return Token(text=text, pos=pos, **defaults)

    # ========================================================================
    # VALIDATOR ADDITIONAL TESTS
    # ========================================================================

    def test_validator_rule_checks_with_flags(self):
        """Test validator rule checks that create flags."""
        tokens = [
            self.create_token("A", PartOfSpeech.ARTICLE),
            self.create_token("cats", PartOfSpeech.NOUN, number=Number.PLURAL),
            self.create_token("runs", PartOfSpeech.VERB),
            self.create_token(".", PartOfSpeech.PUNCTUATION),
        ]
        result = ParseResult(tokens=tokens, flags=[], rule_checks={})

        # Test rules that should create flags
        self.validator._check_rule_1(result)
        self.validator._check_rule_4(result)

        # Check that flags were created
        self.assertGreater(len(result.flags), 0)

    def test_validator_rule_checks_with_no_flags(self):
        """Test validator rule checks that don't create flags."""
        tokens = [
            self.create_token("A", PartOfSpeech.ARTICLE),
            self.create_token("cat", PartOfSpeech.NOUN, number=Number.SINGULAR),
            self.create_token("runs", PartOfSpeech.VERB),
            self.create_token(".", PartOfSpeech.PUNCTUATION),
        ]
        result = ParseResult(tokens=tokens, flags=[], rule_checks={})

        # Test rules that shouldn't create flags
        self.validator._check_rule_1(result)

        # Check that no flags were created
        self.assertEqual(len(result.flags), 0)

    def test_validator_helper_methods(self):
        """Test validator helper methods."""
        from kirkham.models import Phrase

        tokens = [
            self.create_token("John", PartOfSpeech.NOUN),
            self.create_token("and", PartOfSpeech.CONJUNCTION),
            self.create_token("Mary", PartOfSpeech.NOUN),
            self.create_token("play", PartOfSpeech.VERB),
        ]

        # Create a phrase object
        subject_phrase = Phrase(
            tokens=tokens[:3],  # John and Mary
            head_index=0,
            phrase_type="noun_phrase"
        )

        # Test helper methods
        is_compound = self.validator._is_compound_subject(subject_phrase)
        self.assertIsInstance(is_compound, bool)

    def test_validator_edge_cases(self):
        """Test validator edge cases."""
        # Test with tokens that have missing features
        tokens = [
            Token(
                text="test",
                lemma="test",
                pos=PartOfSpeech.NOUN,
                start=0,
                end=4,
                case=None,
                gender=None,
                number=None,
                person=None,
                features={},
            )
        ]
        result = ParseResult(tokens=tokens, flags=[], rule_checks={})

        # Test various rules
        self.validator._check_rule_1(result)
        self.validator._check_rule_2(result)
        self.validator._check_rule_3(result)
        self.validator._check_rule_4(result)

        # Should not crash
        self.assertIsNotNone(result)

    # ========================================================================
    # FORMATTER ADDITIONAL TESTS
    # ========================================================================

    def test_formatter_edge_cases(self):
        """Test formatter edge cases."""
        # Test with empty result
        empty_result = ParseResult(
            tokens=[],
            subject=None,
            verb_phrase=None,
            object_phrase=None,
            voice=None,
            tense=None,
            sentence_type=None,
            rule_checks={},
            flags=[],
            errors=[],
            warnings=[],
            notes=[],
        )

        # Test all formatter methods with empty result
        json_str = self.formatter.to_json(empty_result)
        conll_str = self.formatter.to_conll(empty_result)
        treebank_str = self.formatter.to_penn_treebank(empty_result)
        graphviz_str = self.formatter.to_graphviz(empty_result)
        formatted = self.formatter.format_text(empty_result)

        # Should not crash
        self.assertIsInstance(json_str, str)
        self.assertIsInstance(conll_str, str)
        self.assertIsInstance(treebank_str, str)
        self.assertIsInstance(graphviz_str, str)
        self.assertIsInstance(formatted, str)

    def test_formatter_with_complex_features(self):
        """Test formatter with complex grammatical features."""
        tokens = [
            self.create_token(
                "She",
                PartOfSpeech.PRONOUN,
                case=Case.NOMINATIVE,
                number=Number.SINGULAR,
                person=Person.THIRD,
                gender=Gender.FEMININE,
                features={"is_pronoun": True, "is_personal": True},
            ),
            self.create_token(
                "has",
                PartOfSpeech.VERB,
                features={"is_verb": True, "is_auxiliary": True, "tense": "present"},
            ),
            self.create_token(
                "been",
                PartOfSpeech.VERB,
                features={"is_verb": True, "is_auxiliary": True, "tense": "past"},
            ),
            self.create_token(
                "working",
                PartOfSpeech.VERB,
                features={"is_verb": True, "is_participle": True},
            ),
        ]
        result = ParseResult(
            tokens=tokens,
            subject=None,
            verb_phrase=None,
            object_phrase=None,
            voice=Voice.ACTIVE,
            tense=Tense.PRESENT_PERFECT,
            sentence_type=SentenceType.DECLARATIVE,
            rule_checks={},
            flags=[],
            errors=[],
            warnings=[],
            notes=[],
        )

        # Test all formatter methods
        json_str = self.formatter.to_json(result)
        conll_str = self.formatter.to_conll(result)
        treebank_str = self.formatter.to_penn_treebank(result)
        graphviz_str = self.formatter.to_graphviz(result)
        formatted = self.formatter.format_text(result)

        # Should not crash
        self.assertIsInstance(json_str, str)
        self.assertIsInstance(conll_str, str)
        self.assertIsInstance(treebank_str, str)
        self.assertIsInstance(graphviz_str, str)
        self.assertIsInstance(formatted, str)

    # ========================================================================
    # PARSER ADDITIONAL TESTS
    # ========================================================================

    def test_parser_file_operations(self):
        """Test parser file operations."""
        # Test parse_file with temporary file
        with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".txt") as f:
            f.write("The cat sat on the mat.\n")
            f.write("She gave me the book.\n")
            temp_file = f.name

        try:
            # Test with sentence_per_line=True
            results = self.parser.parse_file(temp_file, sentence_per_line=True)
            self.assertIsInstance(results, list)
            self.assertEqual(len(results), 2)

            # Test with sentence_per_line=False
            results = self.parser.parse_file(temp_file, sentence_per_line=False)
            self.assertIsInstance(results, list)
            self.assertGreater(len(results), 0)
        finally:
            Path(temp_file).unlink(missing_ok=True)

    def test_parser_batch_operations(self):
        """Test parser batch operations."""
        sentences = ["The cat sat.", "The dog barked.", "Birds fly."]

        # Test parse_batch without parallel
        results = self.parser.parse_batch(sentences, parallel=False)
        self.assertIsInstance(results, list)
        self.assertEqual(len(results), 3)

        # Test parse_batch with parallel
        results = self.parser.parse_batch(sentences, parallel=True)
        self.assertIsInstance(results, list)
        self.assertEqual(len(results), 3)

    def test_parser_performance_tests(self):
        """Test parser performance."""
        import time

        # Test single sentence performance
        start_time = time.time()
        result = self.parser.parse("The quick brown fox jumps over the lazy dog.")
        end_time = time.time()

        self.assertLess(end_time - start_time, 1.0)
        self.assertIsInstance(result, ParseResult)

        # Test batch performance
        sentences = ["The cat sat."] * 50
        start_time = time.time()
        results = self.parser.parse_batch(sentences)
        end_time = time.time()

        self.assertLess(end_time - start_time, 5.0)
        self.assertEqual(len(results), 50)

    def test_parser_error_handling(self):
        """Test parser error handling."""
        # Test with various edge cases
        edge_cases = [
            "",  # Empty string
            "   ",  # Whitespace only
            "Test with Ã©mojis ðŸŽ‰",  # Unicode
            "Price: $29.99 (50% off!)",  # Numbers and symbols
            "What? I said: \"Hello!\"",  # Complex punctuation
        ]

        for case in edge_cases:
            result = self.parser.parse(case)
            self.assertIsInstance(result, ParseResult)

    # ========================================================================
    # CLASSIFIER ADDITIONAL TESTS
    # ========================================================================

    def test_classifier_error_handling(self):
        """Test classifier error handling."""
        # Test with various edge cases
        edge_cases = [
            "",  # Empty string
            " ",  # Whitespace
            "123",  # Number
            "@#$",  # Special characters
            "xyzzy",  # Unknown word
        ]

        for case in edge_cases:
            try:
                result = self.classifier.classify(case, context=None)
                self.assertIsNotNone(result)
            except Exception:
                # Some cases may raise exceptions, which is expected
                pass

    def test_classifier_context_variations(self):
        """Test classifier with various context variations."""
        word = "cat"
        contexts = [
            None,  # No context
            [],  # Empty context
            ["the"],  # Single word context
            ["the", "quick", "brown", "fox"],  # Multiple word context
            ["hello", "!", "how", "are", "you"],  # Context with punctuation
        ]

        for context in contexts:
            try:
                result = self.classifier.classify(word, context=context)
                self.assertIsNotNone(result)
            except Exception:
                # Some cases may raise exceptions, which is expected
                pass

    def test_classifier_context_helper_methods(self):
        """Test classifier context helper methods."""
        # Test with various contexts
        contexts = [
            ["its"],  # Should trigger noun context for 'like'
            ["the"],  # Should trigger noun context for 'work'
            ["the"],  # Should trigger noun context for 'wrong'
            ["looks"],  # Should not trigger noun context for 'like'
            ["I"],  # Should not trigger noun context for 'work'
            ["is"],  # Should not trigger noun context for 'wrong'
        ]

        for context in contexts:
            try:
                result1 = self.classifier._is_like_noun_context(context)
                result2 = self.classifier._is_work_noun_context(context)
                result3 = self.classifier._is_wrong_noun_context(context)

                self.assertIsInstance(result1, bool)
                self.assertIsInstance(result2, bool)
                self.assertIsInstance(result3, bool)
            except Exception:
                # Some cases may raise exceptions, which is expected
                pass

    # ========================================================================
    # INTEGRATION TESTS
    # ========================================================================

    def test_full_integration(self):
        """Test full integration of all components."""
        # Test a complex sentence through the full pipeline
        sentence = "The quick brown fox jumps over the lazy dog."

        # Parse the sentence
        result = self.parser.parse(sentence)
        self.assertIsInstance(result, ParseResult)
        self.assertGreater(len(result.tokens), 0)

        # Test formatter methods
        json_str = self.formatter.to_json(result)
        conll_str = self.formatter.to_conll(result)
        treebank_str = self.formatter.to_penn_treebank(result)
        graphviz_str = self.formatter.to_graphviz(result)
        formatted = self.formatter.format_text(result)

        # All should be valid strings
        self.assertIsInstance(json_str, str)
        self.assertIsInstance(conll_str, str)
        self.assertIsInstance(treebank_str, str)
        self.assertIsInstance(graphviz_str, str)
        self.assertIsInstance(formatted, str)

    def test_consistency_across_runs(self):
        """Test consistency across multiple runs."""
        sentence = "The cat sat on the mat."
        results = []

        # Run multiple times
        for _ in range(5):
            result = self.parser.parse(sentence)
            results.append(result)

        # All results should be identical
        for i in range(1, len(results)):
            self.assertEqual(len(results[i].tokens), len(results[0].tokens))
            for j, token in enumerate(results[i].tokens):
                self.assertEqual(token.text, results[0].tokens[j].text)
                self.assertEqual(token.pos, results[0].tokens[j].pos)

    def test_memory_usage(self):
        """Test memory usage doesn't grow excessively."""
        import gc

        # Parse many sentences
        for _ in range(100):
            result = self.parser.parse("The cat sat on the mat.")
            del result

        # Force garbage collection
        gc.collect()

        # Should not crash
        result = self.parser.parse("The cat sat on the mat.")
        self.assertIsInstance(result, ParseResult)


if __name__ == "__main__":
    unittest.main()
